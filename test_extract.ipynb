{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from src.main_function   import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Specify setting file \n",
    "parameter_file = './setting/parameter1.yaml'\n",
    "params = load_parameters(parameter_file)\n",
    "dataset = eval(params['general']['development_dataset'])(data_path=params['path']['data'])\n",
    "params['features']['mfcc']['win_length'] = int(params['features']['win_length_seconds'] * params['features']['fs'])\n",
    "params['features']['mfcc']['hop_length'] = int(params['features']['hop_length_seconds'] * params['features']['fs'])\n",
    "challenge_dataset = eval(params['general']['challenge_dataset'])(data_path=params['path']['data'])\n",
    "result_path = params['path']['results']\n",
    "files = []\n",
    "for item_id, item in enumerate(dataset.train()):\n",
    "    if item['file'] not in files:\n",
    "        files.append(item['file'])\n",
    "files = sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset' feature_extract\")\n",
    "if not os.path.exists(\"save_data/features\"):\n",
    "    do_feature_extraction(files=files,\n",
    "                      dataset=dataset,\n",
    "                      feature_path=params['path']['features'],\n",
    "                      params=params['features'],\n",
    "                      overwrite=params['general']['overwrite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset's Feature normalize\")\n",
    "#if not os.path.exists(\"save_data/features\"):\n",
    "do_feature_normalization(dataset=dataset,\n",
    "                         feature_normalizer_path=params['path']['feature_normalizers'],\n",
    "                         feature_path=params['path']['features']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start trainning by train dataset\")\n",
    "#if not os.path.exists(\"save_data/features\"):\n",
    "do_system_training(dataset=dataset,\n",
    "                   model_path=params['path']['models'],\n",
    "                   feature_normalizer_path=params['path']['feature_normalizers'],\n",
    "                   feature_path=params['path']['features'],\n",
    "                   feature_params=params['features'],\n",
    "                   classifier_params=params['classifier']['parameters'],\n",
    "                   classifier_method=params['classifier']['method'],\n",
    "                   clean_audio_errors=params['classifier']['audio_error_handling']['clean_data'],\n",
    "                   overwrite=params['general']['overwrite']\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"create challenge dataset\")\n",
    "challenge_dataset = eval(params['general']['challenge_dataset'])(data_path=params['path']['data'])\n",
    "result_path = params['path']['results']\n",
    "files = []\n",
    "for item_id, item in enumerate(challenge_dataset.test()):\n",
    "    if item['file'] not in files:\n",
    "        files.append(item['file'])\n",
    "files = sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test dataset feature extract')\n",
    "do_feature_extraction(files=files,\n",
    "                        dataset=challenge_dataset,\n",
    "                        feature_path=params['path']['features'],\n",
    "                        params=params['features'],\n",
    "                        overwrite=params['general']['overwrite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('classify test dataset')\n",
    "do_system_testing(dataset=challenge_dataset,\n",
    "                                    feature_path=params['path']['features'],\n",
    "                                     result_path=result_path,\n",
    "                                     model_path=params['path']['models'],\n",
    "                                    feature_params=params['features'],\n",
    "                                    classifier_method=params['classifier']['method'],\n",
    "                                    overwrite=params['general']['overwrite'] or params['general']['challenge_submission_mode']\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"evaluation\")\n",
    "do_system_evaluation(dataset=challenge_dataset,\n",
    "                     result_path=result_path)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
